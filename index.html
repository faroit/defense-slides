<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/alabs.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<script src="https://unpkg.com/wavesurfer.js"></script>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
	<div class="reveal">
	<div class="slides">

	<section class="cover" data-background="css/theme/alabs/alabs_header.jpg" data-background-repeat="no-repeat" data-background-size="100%" data-background-position="0 0" data-state="no-title-footer no-progressbar">
		<h2>Separation and Count Estimation for Audio Sources Overlapping in Time and Frequency</h2>
		<h3>PhD. Defense</h3>
		<p>
			Fabian-Robert StÃ¶ter<br />
		</p>
		<p>
			September 19th, 2019
		</p>
		<p>
			<div class='logos'>
				<img src="css/theme/alabs/FAU_bw.svg" id="FAU" class="logo" alt="">
				<img src="css/theme/alabs/IIS_bw.svg" id="IIS" class="logo" alt="">
			</div>
	</section>

	<section data-state="no-title-footer no-progressbar" data-background="black" data-state="no-title-footer" data-fullscreen>
		<div class="container">
			<div class="col fragment">
				<h1 style="color:#ED8C01">How many speakers can you hear?</h1>
				<div>
					<span class="fragment" data-audio-src="assets/audio/two_speaker_faded.wav"><h3>Example A</h3></span>
				</div>
				<div>
					<span class="fragment" data-audio-src="assets/audio/ten_speakers_faded.wav"><h3>Example B</h3></span>
				</div>
			</div>

			<div class="col fragment">
				<h1 style="color:lawngreen; margin-top: 8em">Can you hear out each instrument?</h1>
				<div>
					<span class="fragment" data-audio-src="assets/audio/jazz_wice.wav">
						<h3>Example A</h3>
					</span>
				</div>
				<div>
					<span class="fragment" data-audio-src="assets/audio/unison_wice.wav">
						<h3>Example B</h3>
					</span>
				</div>
			</div>
		</div>
	</section>

	<section>
		<h2>Overlap in Time and Frequency</h2>
		<div class="container">
			<div class="col">
				<img data-audio-src="assets/audio/00.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_00.svg" alt=""></br>
				<img class="fragment fade-in-then-semi-out" data-audio-src="assets/audio/01.wav" src="assets/images/overlap_01.svg" alt="">
			</div>
			<div class="col">
				<img data-audio-src="assets/audio/10.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_10.svg" alt=""></br>
				<img data-audio-src="assets/audio/11.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_11.svg" alt="">
			</div>
		</div>
	</section>

	<section>
		<h2>The big picture</h2>
		
		<div style="text-align: center"><img width="650px" src="assets/images/mixture_model.svg" alt=""></div>
		<p></p>
		<div class="container">
			<div class="col" style='text-align: center'>
				<h3>Signal Processing</h3>
				<ul>
					<li>Can we obtain $\mathbf{s}_j$ from $\mathbf{x}$?</li>
					<li>Source Separation</li>
				</ul>
			</div>
			<div class="col" style='text-align: center'>
				<h3>Signal Analysis</h3>
				<ul>
					<li>Can we find $k$ from $\mathbf{x}$?</li>
					<li>Count Estimation</li>
				</ul>
			</div>
		</div>
	</section>

	<section>
		<h2>Applications</h2>
		<div class="container">
			<div class="col">
			<h3>Separation</h3>
				<ul>
					<li>Active Listening</li>
					<li>Hearing Aids</li>
					<li>Frontend for many other signal processing methods</li>
				</ul>
			</div>
			<div class="col">
				<h3>Count Estimation</h3>
				<ul>
					<li>First step to address the separation</li>
					<li>Crowd Surveillance</li>
					<li>Wildlife Monitoring</li>
				</ul>
			</div>
		</div>
	</section>

	<section data-background="#ED8C01" data-state="no-title-footer">
		<h1 style='margin-top: 200px; color: white; font-size: 2.4em'>Source Separation</h1>
	</section>

	<!-- <section>
		<h2>Scenario: Unison Separation</h2>
		<div class="container">
			<div class="col" style="text-align: center">
						<h3><img width="64px" src="assets/images/trumpet_icon.png" alt=""></h3>
						<div style="text-align: center">
							<img width="100%" src="assets/images/trumpet.svg">
						</div>

			</div>
		<div class="col">
			<div class="col" style="text-align: center">
				<h3><img width="64px" src="assets/images/cello_icon.png" alt=""></h3>
				<div style="text-align: center">
					<img width="100%" src="assets/images/cello.svg">
				</div>
			
			</div>
		</div>
		</div>
	</section> -->

	<section data-transition="slide fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<h3><img width="32px" src="assets/images/trumpet_icon.png" alt=""> Trumpet</h3>
		<div style="text-align: center" >
			<img width="80%" src="assets/images/trumpet.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<h3><img width="32px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
		<div style="text-align: center">
			<img width="80%" src="assets/images/cello.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<h3><img width="32px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
		<div style="text-align: center">
				<img width="80%" src="assets/images/cello_ani.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<h3><img width="32px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
		<div style="text-align: center">
			<img width="80%" src="assets/images/cello.svg">
		</div>
	</section>

	<section data-transition="fade-in side">
		<h2>Instruments playing Unison</h2>
		<h3><img width="32px" src="assets/images/trumpet_icon.png" alt=""> Trumpet + <img width="32px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
		<div style="text-align: center">
				<img width="80%" src="assets/images/both.svg">
		</div>
	</section>
	
	<section>
		<h2>Research Question</h2>
		<ul>
			<li>Unison has extreme overlap</li>
			<li>Separation Sandbox</li>
		</ul>
	</br>
	</br>
		<p style="width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
			Can we utilize modulations for separation of unison mixtures?
		</p>
			</br>
			</br>

		<h3>Contributions</h3>
		<ul>
			<li>Known/Informed Modulation</li>
			<li>Unknown Modulation</li>
		</ul>
	</section>

	<section>
			<h2>Known Modulation</h2>
			<h3>Modeling Instationary Signals</h3>
	
			<div class="container">
				<div class="col">
					<ul>
						<li>TF Classification <a class='ref' href="#">Hu 2002</a></li>
						<li>Source/filter model <a class='ref' href="#">Durrieu 2010</a></li>
						<li>Spectral Comb Filter <a class='ref' href="#">Cano 2012</a></li>
					</ul>
					<br><br>
					<h3>Proposed</h3>
					<ul>
						<li>Use Time Warping <a class='ref' href="#">StÃ¶ter 2014</a></li></li>
						<li>Filter in time domain
							<ul>
								<li>less artifacts</li>
								<li>required: accurate F0</li>
							</ul>
						</li>
						
					</ul>
				</div>
				<div class="col">
					<img style="float:right" width="100%" src="assets/images/ReviewPaper_ Figure2.svg" alt="">
					<br>
				</div>
			</div>
	</section>

	<section>
		<h2>MUSERC: MUlti SEnsor Recordings: Cello</h2>
		<video controls autoplay loop>
			<source data-src="assets/video/cello.mp4" type="video/mp4" />
		</video>
	</section>

	<section>
		<h2>Unknown Modulation</h2>
		<h3>Modeling Instationary Signals</h3>
	
		<div class="container">
			<div class="col">
				<ul>
					<li>HR-NMF <a class='ref' href="#">Baedau 2011</a></li>
					<li>Modulation Tensor <a class='ref' href="#">Barker 2013</a></li>
				</ul>
				<br><br>
				<h3>Proposed</h3>
				<ul>
					<li>Novel Representation</li>
					<li>Suitable Separation Model</li>
				</ul>
			</div>
			<div class="col">
				<img width="60%" src="assets/images/ReviewPaper_ Figure3.svg" alt="">
				<br>
			</div>
		</div>
	</section>

	<section data-background="assets/images/flocks.png" data-state="no-title-footer">
		<p style="background-color:rgba(255, 255, 255, .7) ; padding: 10px; color:black; margin-top: 15em">
			<strong>Common Fate:</strong> groups common modulation textures to sources.
		</p>
	
	</section>

	<section data-background-video="assets/video/flocks.mov" data-state="no-title-footer" data-background-video-muted>
		<p style="background-color:rgba(255, 255, 255, .7) ; padding: 10px; color:black; margin-top: 15em">
			<strong>Common Fate:</strong> groups common modulation textures to sources.
		</p>

	</section>

	<section>
		<h2>Step 1: Short-Time Fourier Transform</h2>
		<!-- TODO PLAY -->
		<div style="text-align: center">
				<img style="margin-left:-20px" width="70%" src="assets/images/stft.png">
		</div>
		<p style="text-align:center">
			$\mathbf{X} \in \mathbb{C}^{352 \times 279}$
		</p>
	</section>

	<section>
		<h2>Step 2: Common Fate Transform (CFT)</h2>
		<table>
			<tr>
				<td>
					<img src="assets/images/gft.png" width="100%" alt="" />
					<p style="text-align:center">
						STFT Grid
					</p>
					<p style="text-align:center">
						$\mathcal{G} \in \mathbb{C}^{32 \times 48 \times 11 \times 6}$
					</p>
				</td>
				<td>
					<img src="assets/images/cft.png" width="100%" alt="" />
					<p style="text-align:center">
						CFT
					</p>
					<p style="text-align:center">
						$\mathcal{V} \in \mathbb{C}^{32 \times 48 \times 11 \times 6}$
					</p>
				</td>
			</tr>
		</table>
		<p>
		</p>
	</section>

	<section>
		<h2>In Detail</h2>
		<img class="stretch" src="assets/images/gridplot.svg">
	</section>

	<section>
		<h2>Compared to modulation spectrograms... </h2>
		<ul>
			<li>CFT is computed using complex STFT $X$
				<ul>
					<li>Easily invertible</li>
					<li>Models phase dependencies between neighbouring STFT entries</li>
				</ul>
			</li>
			<li>Patches span/merge several frequency bins</li>
			<li>Results in <strong>modulation texture</strong></li>
		</ul>
		<p></p>
		<h3>Common Fate Model</h3>
		<ul>
			<li>Using Non-Negative Tensor Factorization</li>
		</ul>
	</section>

	<section>
		<section>
			<h2>Demo: Sax + Flute</h2>
			<iframe src="assets/audio/commonfate/66-73.html" class="stretch"></iframe>
		</section>
		<section>
			<h2>Demo: Viola + Flute</h2>
			<iframe src="assets/audio/commonfate/42-73.html" class="stretch"></iframe>
		</section>
	</section>

	<section>
		<h2>Extension music separation</h2>
		<ul>
			<li>Common fate transform + Deep Neural Networks</li>
			<li>Model from Uhlich 2015</li>
			<li>Improvements compared to STFT</li>
		</ul>
		<p></p>
		<img width="80%" src="assets/images/uhlich_dnn.svg" alt="">
	</section>

		<section>
			<h2>Extension for real world data</h2>
			<h3>Results SiSEC 2016</h3>
			<img src="assets/images/sisec17.svg" alt="">
			<ul>
				<li><a href="https://www.sisec17.audiolabs-erlangen.de/#/listen/21/STO1">DEMO</a></li>
			</ul>
		</section>


	<section>
		<h2>Summary</h2>
		<h3>Processing Methods for Separation</h3>
		<ul>
			<li>Unison Source Separation Scenario</li>
			<li>Importance of Modulations
				<ul>
					<li>Informed by $F_0$ to apply time warping</li>
					<li>Common Fate Representation for unknown modulations</li>
					<li>Common Fate Model, a suitable Factorization Model</li>
					<li>Deep Common Fate, supervised separation</li>
				</ul>
			</li>
		</ul>
	</section>


	<section data-background="#ED8C01" data-state="no-title-footer">
		<h1 style='color:white; font-size: 2.4em'>Source Count Estimation</h1>
	</section>

	<section>
		<h2>Strategies to Count</h2>
		<div class="container">
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_detection.svg" alt=""></br>
			</div>
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_estimation.svg" alt="">
			</div>
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_subitizing.svg" alt="">
			</div>
		</div>
	</section>

	<section>
		<h2>Research Questions</h2>
		<ul>
			<li>What are the limitations of subitizing for humans?</li>
			<li>Can we directly count/subitize audio sources?</li>
			<li>Can we build a machine to solve these tasks?</li>
			<li>Would a machine be subject to the same limitations?</li>
		</ul>
	</section>

	<section>
		<h2>Experiment I</h2>
		<h3>What is the number of concurrent speakers?</h3>
			<div class="container">
				<div class='col' style="padding-top: 1em">
					<ul>
						<li>Experiment with English</li>
						<li>n=40</li>
						<li>"One-Two-Three-(Four)-Many"</li>
						<li><a href="https://denumerate.app">https://denumerate.app</a></li>
					</ul>				
				</div>
				<div class='col' style="padding-top: 1em">
					<img src="assets/images/speech_experiment.svg" alt="">
				</div>
			</div>
	
		</section>

		<section>
			<h2>Experiment II</h2>
			<h3>What is the number of instruments in music?</h3>
			<div class="container">
				<div class='col' style="padding-top: 1em">
					<ul>
						<li>12 music stimuli</li>
						<li>Unison mostly detected</li>
						<li>Musicians vs. Non-Musicians</li>
						<li>First large-scale web-audio based experiment (n=1168)</li>
					</ul>
				</div>
				<div class='col' style="padding-top: 1em">
					<img src="assets/images/count_instr.svg" alt="">
				</div>
			</div>
		</section>

		<section>
			<h2>Results I</h2>

			<div class="container">
				<div class="col">
					<ul style="font-size: 24px">
						<li class="fragment">Excellent voice activity detection</li>
						<li class="fragment">Overestimation between $2 < k < 6$</li>
						<li class="fragment">Error rarely larger than $k=2$</li></k>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/responses.svg" alt="">			
				</div>
				
			</div>
		</section>

		<section>
			<h2>Input and Output</h2>
			<img width="100%" src="assets/images/io.svg" alt="">
		</section>

		<section>
			<section>
				<h2>Architectures</h2>
				<img width="120%" src="assets/images/architectures.svg" alt="">
		
			</section>
			<section>
				<h2>Architectures</h2>
				<img width="120%" src="assets/images/networkoverview.svg" alt="">
		
			</section>
			<section>
				<h2>CRNN</h2>
				<img width="120%" src="assets/images/nn.svg" alt="">
			</section>
		</section>

		<section>
			<h2>Superhuman</h2>
			<div class="container">
				<div class="col">
					<ul style="font-size: 24px">
						<li class="fragment">Excellent voice activity detection</li>
						<li class="fragment">Overestimation between $2 < k < 6$</li> <li class="fragment">Error rarely larger
								than $k=2$</li>
						</k>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/superhuman.svg" alt="">
				</div>
			</div>
		</section>


		<section>
			<h2>Complexity</h2>
			<div class="container">
				<div class="col">
					<ul style="font-size: 24px">
						<li class="fragment">Excellent voice activity detection</li>
						<li class="fragment">Overestimation between $2 < k < 6$</li> <li class="fragment">Error rarely larger
								than $k=2$</li>
						</k>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/complexity.svg" alt="">
				</div>
			</div>
		</section>

		<section>
			<h2>Results</h2>

			<div class="container">
				<div class='col'>
					<div class="fig-container" data-file="assets/figures/original.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech
					</h4>

				</div>
				<div class='col fragment step-fade-in-then-out'>
				<ul style='font-size: 0.8em'>
					<li>MEAN: $k=5$</li>
					<li>VQ: K-Means of ${MFCC}[7]$</li>
					<li>SVC: SVM Classification ${MFCC}_{20}$</li>
					<li>SVR: SVM Regression ${MFCC}_{20}$</li>
					<li>RNN: DNN Recurrent LSTM on STFT</li>
					<li>CRNN: DNN LSTM+CNN on STFT</li>
				</ul>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/timit.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: TIMIT
					</h4>

				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/chinese.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: THCS10 ðŸ‡¨ðŸ‡³
					</h4>				
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/gains.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech +/- 6dB
					</h4>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/rev.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech Reverberated
					</h4>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/revrev.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech Reverberated</br>
						Test: LibriSpeech Reverberated
					</h4>
				</div>
			</div>				
		</section>

		<section>
			<h2>CountNet Demo</h2>
			<video controls loop>
				<source data-src="assets/video/countnet-demo.mp4" type="video/mp4" />
			</video>
		</section>

		<section>
			<h2>Model selection</h2>
			<img src="assets/images/countnet_parameters.svg" alt="">
					<p class="fragment" style="width: 70%; margin: auto; background-color: orange; padding: 10px; color:white">
						CountNet = STFT > CRNN > Classification
					</p>

		</section>

		<section>
			<h2>Architectures</h2>
			<div class="container">
				<div class="col"></div>
				<div class="col">
					<img width="120%" src="assets/images/task.svg" alt="">
				</div>
			</div>

		</section>

		<section data-background="assets/images/blackbox.jpg" data-state="no-title-footer">
		
		</section>

		<section>
			<h2>Understanding CountNet</h2>
			<h3 style="margin-top: -20px">Convolutional Filters</h3>
			<img width="120%" src="assets/images/convfilters.svg" alt="">
			<p style="font-size: 24px; width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
				Does CountNet understand phonemes?
			</p>

		</section>

		<section>
			<h2>Understanding CountNet</h2>
			<h3 style="margin-top: -20px">Ablation Study</h3>
			<ul>
				<li>Using phonetic annotation in TIMIT dataset</li>	
				<li>Measure speaking rate</li>
				<li>Syllables per second => Modulation of Speech</li>
			</ul>
			<img width="90%" src="assets/images/phonemes.svg" alt="">
		</section>

		<section>
			<h2>Understanding CountNet</h2>
			<h3 style="margin-top: -20px">Results</h3>
			<div class="container">
				<div class="col">
					<ul>
						<li>Retrained model on TIMIT</li>
						<li>Select k=6 items (balanced)</li>
						<li>Errors only <emph>underestimation</emph></li>
						<li>Significant effect of speaking rate</li>
						<li>> Speaking very slow may result in understimation</li>
					</ul>
				<p style="font-size: 24px; width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
					CountNet used speech modulations
				</p>
				</div>

				<div class="col" style="flex: 0.5">
					<img width="70%" src="assets/images/speaking_rate.svg" alt="">
				</div>			
			</div>
		</section>

		
		<section>
			<h2>Conclusion</h2>
			<ul>
				<li>Proposed two methods for separating unison mixtures</li> 
				<li>Extended Common Fate for the Music scenario</li>
				<li>First attempt to model speaker count estimation using DL</li>
				<li>Listening tests showed performance better already â€œsuper-human"</li>
				<li>Modulations speech</li>
			</ul>
		</section>

	<section>
		<h2>Further Contributions</h2>
		<div class="container">
			<div class="col" style="flex: 2">
				<h3>Data</h3>
				<ul>
					<li>Unison Dataset <a href="https://doi.org/10.5281/zenodo.1467921"><img
								src="https://zenodo.org/badge/DOI/10.5281/zenodo.1467921.svg" alt="DOI"></a>
					</li>
					<li>Cello Dataset <a href="https://doi.org/10.5281/zenodo.1560651"><img
								src="https://zenodo.org/badge/DOI/10.5281/zenodo.1560651.svg" alt="DOI"></a></li>
					<li>DSD100, MUSDB18 Dataset</li>
				</ul>
				<h3>Software Packages</h3>
				<ul>
					<li><a href="http://github.com/aliutkus/commonfate">commonfate</a></li>
					<li>musdb</li>
					<li>museval</li>
				</ul>
				<h3>Community</h3>
				<ul>
					<li>SiSEC Organization 2016, 2018</li>
					<li><a href="http://sisec17.audiolabs-erlangen.de">Interactive Evaluation</a></li>
				</ul>
	
			</div>
	
			<div class="col">
	
			</div>
		</div>
	</section>
	<section>
		<img width="120%" src="assets/images/trends.svg" alt="">
	</section>

	</div>
	<div class='footer'>
		<div id="copyright">Fabian-Robert StÃ¶ter</br>PhD Defense</div>
		<img src="css/theme/alabs/alabs_logo.svg" alt="Logo" />
		<div id="middlebox">Separation and Count Estimation for Audio Sources</br> Overlapping in Time and Frequency</div>
	</div>
	</div>

	<script src="js/reveal.js"></script>

	<script>
		// More info about config & dependencies:
		// - https://github.com/hakimel/reveal.js#configuration
		// - https://github.com/hakimel/reveal.js#dependencies
		// var wavesurfer = WaveSurfer.create({
		// 		container: '#waveform',
		// 		waveColor: 'grey',
		// 		progressColor: 'orange'
		// 	});
		// 	wavesurfer.load('assets/audio/fm_sine.wav');

		Reveal.initialize({
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				TeX: {
					extensions: ["color.js"]
				}
			},
				// specified using percentage units.
			width: 960,
			height: 700,
			controls: false,
			progress: true,
			history: true,
			center: false,
			slideNumber: true,
			minScale: 0.1,
			maxScale: 5,
			transition: 'none', //
			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.wav',		// audio files have the ".ogg" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: -10, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
				autoplay: true,	// automatically start slideshow
				defaultDuration: 10,	// default duration in seconds if no audio is available 
				defaultAudios: false,	// try to play audios with names such as audio/1.2.ogg
			},
			dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'node_modules/reveald3/reveald3.js' },
				{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function () { return !!document.body.classList; } },
				{ src: 'plugin/highlight/highlight.js', async: true }
			]
		});

	</script>
	</body>
</html>
