<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/alabs.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<script src="https://unpkg.com/wavesurfer.js"></script>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
	<div class="reveal">
	<div class="slides">

	<section class="cover" data-background="css/theme/alabs/alabs_header.jpg" data-background-repeat="no-repeat" data-background-size="100%" data-background-position="0 0" data-state="no-title-footer no-progressbar">
		<h2>Separation and Count Estimation for Audio Sources Overlapping in Time and Frequency</h2>
		<h3>PhD. Defense</h3>
		<p>
			Fabian-Robert StÃ¶ter<br />
		</p>
		<p>
			September 19th, 2019
		</p>
		<p>
			<div class='logos'>
				<img src="css/theme/alabs/FAU_bw.svg" id="FAU" class="logo" alt="">
				<img src="css/theme/alabs/IIS_bw.svg" id="IIS" class="logo" alt="">
			</div>
	</section>

	<section data-state="no-title-footer no-progressbar" data-background="black" data-state="no-title-footer" data-fullscreen>
		<div class="container">
			<div class="col fragment">
				<h1 style="color:#ED8C01">How many speakers can you hear?</h1>
				<div>
					<span class="fragment" data-audio-src="assets/audio/two_speaker_faded.wav"><h3>Example A</h3></span>
				</div>
				<div>
					<span class="fragment" data-audio-src="assets/audio/ten_speakers_faded.wav"><h3>Example B</h3></span>
				</div>
			</div>

			<div class="col fragment">
				<h1 style="color:lawngreen; margin-top: 6em">Can you hear out each instrument?</h1>
				<div>
					<span class="fragment" data-audio-src="assets/audio/jazz_wice.wav">
						<h3>Example A</h3>
					</span>
				</div>
				<div>
					<span class="fragment" data-audio-src="assets/audio/unison_wice.wav">
						<h3>Example B</h3>
					</span>
				</div>
			</div>
		</div>
	</section>

	<section>
		<h2>Overlap in Time and Frequency</h2>
		<div class="container">
			<div class="col">
				<img data-audio-src="assets/audio/00.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_00.svg" alt=""></br>
				<img class="fragment fade-in-then-semi-out" data-audio-src="assets/audio/01.wav" src="assets/images/overlap_01.svg" alt="">
			</div>
			<div class="col">
				<img data-audio-src="assets/audio/10.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_10.svg" alt=""></br>
				<img data-audio-src="assets/audio/11.wav" class="fragment fade-in-then-semi-out" src="assets/images/overlap_11.svg" alt="">
			</div>
		</div>
	</section>

	<section>
		<h2>The big picture</h2>
		
		<div style="text-align: center"><img width="650px" src="assets/images/mixture_model.svg" alt=""></div>
		<p></p>
		<div class="container">
			<div class="fragment col" style='border:5px solid#00ADD8; padding: 0.6em; text-align: center'>
				<h3>Signal Processing</h3>
				<ul>
					<li>Can we obtain $\mathbf{s}_j$ from $\mathbf{x}$?</li>
					<li class='confirm'>Source Separation</li>
				</ul>
			</div>
			<div class="fragment col" style='border: 5px solid#F74949; padding: 0.6em; text-align: center'>
				<h3>Signal Analysis</h3>
				<ul>
					<li>Can we find $k$ from $\mathbf{x}$?</li>
					<li class="confirm">Count Estimation</li>
				</ul>
			</div>
		</div>
	</section>

	<section>
		<h2>Applications</h2>
		<div class="container">
			<div class="col">
			<h3>Separation</h3>
				<ul>
					<li>Active Listening</li>
					<li>Hearing Aids</li>
					<li>Frontend for many other signal processing methods</li>
				</ul>
			</div>
			<div class="col">
				<h3>Count Estimation</h3>
				<ul>
					<li>First step to address the separation</li>
					<li>Crowd Surveillance</li>
					<li>Wildlife Monitoring</li>
				</ul>
			</div>
		</div>
	</section>

	<section data-background="#00ADD8" data-state="no-title-footer">
		<h1 style='margin-top: 200px; color: white; font-size: 2.4em'>Source Separation</h1>
	</section>

	<section data-transition="slide fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<div style="text-align: center" >
				<h3><img width="48px" src="assets/images/trumpet_icon.png" alt=""> Trumpet</h3>
			<img width="80%" src="assets/images/trumpet.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<div style="text-align: center">
				<h3><img width="48px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
			<img width="80%" src="assets/images/cello.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<div style="text-align: center">
				<h3><img width="48px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
				<img width="80%" src="assets/images/cello_ani.svg">
		</div>
	</section>

	<section data-transition="fade-in fade-out">
		<h2>Scenario: Instruments playing Unison</h2>
		<div style="text-align: center">
				<h3><img width="48px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
			<img width="80%" src="assets/images/cello.svg">
		</div>
	</section>

	<section data-transition="fade-in side">
		<h2>Instruments playing Unison</h2>
		<div style="text-align: center">
				<h3><img width="32px" src="assets/images/trumpet_icon.png" alt=""> Trumpet + <img width="32px" src="assets/images/cello_icon.png" alt=""> Cello</h3>
				<img width="80%" src="assets/images/both.svg">
		</div>
	</section>
	
	<section>
		<h2>Research Question</h2>
		<ul>
			<li>Unison has extreme overlap</li>
			<li>Separation Sandbox</li>
		</ul>
	</br>
	</br>
		<p style="width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
			Can we utilize modulations for separation of unison mixtures?
		</p>
			</br>
			</br>

		<h3>Contributions</h3>
		<ul>
			<li>Known/Informed Modulation</li>
			<li>Unknown Modulation</li>
		</ul>
	</section>

	<section>
			<h2>Known Modulation</h2>
			<h3>Modeling Instationary Signals</h3>
	
			<div class="container">
				<div class="col">
					<ul>
						<li>TF Classification</li>
						<li>Source/filter model</li>
						<li>Spectral Comb Filter</li>
					</ul>
					<br><br>
					<h3>Proposed</h3>
					<ul>
						<li>Use Time Warping</li></li>
						<li>Filter in time domain
							<ul>
								<li>less artifacts</li>
								<li>required: accurate F0</li>
							</ul>
						</li>
						
					</ul>
				</div>
				<div class="col">
					<img style="float:right" width="100%" src="assets/images/ReviewPaper_ Figure2.svg" alt="">
					<br>
				</div>
			</div>
	</section>

	<section>
		<h2>MUSERC: MUlti SEnsor Recordings: Cello</h2>
		<video controls autoplay loop>
			<source data-src="assets/video/cello.mp4" type="video/mp4" />
		</video>
	</section>

	<section>
		<h2>Unknown Modulation</h2>
		<h3>Modeling Instationary Signals</h3>
	
		<div class="container">
			<div class="col">
				<ul>
					<li>HR-NMF</li>
					<li>Modulation Tensor</li>
				</ul>
				<br><br>
				<h3>Proposed</h3>
				<ul>
					<li>Novel Representation</li>
					<li>Suitable Separation Model</li>
				</ul>
			</div>
			<div class="col">
				<img width="60%" src="assets/images/ReviewPaper_ Figure3.svg" alt="">
				<br>
			</div>
		</div>
	</section>

	<section data-transition="none" data-background="assets/images/flocks.png" data-state="no-title-footer">
		<p style="background-color:rgba(255, 255, 255, .7) ; padding: 10px; color:black; margin-top: 15em">
			<strong>Common Fate:</strong> groups based on their common motion over time.</p>
	
	</section>

	<section data-transition="none" data-background-video="assets/video/flocks.mov" data-state="no-title-footer" data-background-video-muted>
		<p style="background-color:rgba(255, 255, 255, .7) ; padding: 10px; color:black; margin-top: 15em">
			<strong>Common Fate:</strong> groups based on their common motion over time.
		</p>

	</section>

		<section>
			<h2>Common Fate Transform (CFT)</h2>
			<!-- TODO PLAY -->
			<div style="text-align: center">
				<img class="stretch" src="assets/images/cft_blocks_streamlined.svg">
			</div>
			<ul>
				<li>CFT is computed using complex STFT</li>
				<li>Easily invertible</li>
				<li>Results is <strong>modulation texture</strong></li>
			</ul>
		</section>


	<section>
		<h2>In Detail</h2>
		<img class="stretch" src="assets/images/gridplot.svg">
	</section>

	<section>
		<h2>Common Fate Model (CFM)</h2>
			<p>
				<img src="assets/images/cfm_unfolded.svg" width="80%" alt="" />
				$$\sum\limits_{j=1}^{J} \mathcal{A}_{j}(a,b,f) \circ \mathbf{h}_{j}(t)$$
			</p>
			<ul>
				<li>Based on Non-Negative Tensor Factorization</li>
			</ul>
	</section>

	<section>
		<section>
			<h2>Demo: Sax + Flute</h2>
			<iframe src="assets/audio/commonfate/66-73.html" class="stretch"></iframe>
		</section>
		<section>
			<h2>Demo: Viola + Flute</h2>
			<iframe src="assets/audio/commonfate/42-73.html" class="stretch"></iframe>
		</section>
	</section>

	<section>
		<h2>Extension music separation</h2>
		<ul>
			<li>Common fate transform + Deep Neural Networks</li>
			<li>Model from <a href="#">Uhlich 2015</a></li>
			<li>Improvements compared to STFT</li>
		</ul>
		<p></p>
		<img width="80%" src="assets/images/uhlich_dnn.svg" alt="">
	</section>

		<section>
			<h2>Extension for real world data</h2>
			<h3>Results SiSEC 2016</h3>
			<img src="assets/images/sisec17.svg" alt="">
			<ul>
				<li><a href="https://www.sisec17.audiolabs-erlangen.de/#/listen/21/STO1">DEMO</a></li>
			</ul>
		</section>

	<!-- <section>
		<h2>Summary</h2>
		<h3>Processing Methods for Separation</h3>
		<ul>
			<li>Unison Source Separation Scenario</li>
			<li>Importance of Modulations
				<ul>
					<li>Informed by $F_0$ to apply time warping</li>
					<li>Common Fate Representation for unknown modulations</li>
					<li>Common Fate Model, a suitable Factorization Model</li>
					<li>Deep Common Fate, supervised separation</li>
				</ul>
			</li>
		</ul>
	</section>
 -->
	<section data-background="#F74949" data-state="no-title-footer">
		<h1 style='margin-top: 200px; color:white; font-size: 2.4em'>Source Count Estimation</h1>
	</section>

	<section>
		<h2>Strategies to Count</h2>
		<div class="container">
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_detection.svg" alt=""></br>
			</div>
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_estimation.svg" alt="">
			</div>
			<div class='col fragment' style="padding: 1em">
				<img src="assets/images/counting_subitizing.svg" alt="">
			</div>
		</div>

	</section>

	<section>
		<h2>Research Questions</h2>
		<ul>
			<li>What are the limitations of subitizing audio sources?</li>
			<li>Can we build a machine to solve this tasks?</li>
			<li>Would a machine be subject to the same limitations?</li>
		</ul>
	</section>

		<section>
			<h2>Study on Music</h2>
			<h3>What is the number of instruments in music?</h3>
			<div class="container">
				<div class='col' style="font-size: 28px; padding-top: 1em">
					<ul>
						<li>12 music stimuli</li>
						<li>Unison for $k=2$</li>
						<li>Musicians vs. Non-Musicians</li>
						<li>Web based experiment</li>
						<li>lab ($n=40$) vs web ($n=1168$)</li>
					</ul>
				</div>
				<div class='col fragment step-fade-in-then-out'>
					<h1 style="padding: 1em; font-size: 100px">ðŸ”ˆ</h1>
				</div>
				<div class='col fragment step-fade-in-then-out' data-audio-src="assets/audio/jazz_wice.wav">
					<h1 style="padding: 1em; font-size: 100px">ðŸ”Š</h1>
				</div>
				<div class='col fragment step-fade-in-then-out' style="padding-top: 1em">
					<img src="assets/images/count_instr.svg" alt="">
				</div>
			</div>
		</section>

	<section>
		<h2>Study on Speech</h2>
		<h3>What is the number of concurrent speakers?</h3>
			<div class="container">
				<div class='col' style="font-size: 28px; padding-top: 1em">
					<ul>
						<li>100 English stimuli (5s)</li>
						<li>n=40</li>
						<li>$k_{max}$: blind vs informed</li>
						<li>Confirm earlier study in japanese</li>
						<li>Experiment <a href="https://denumerate.app">denumerate.app</a></li>
					</ul>				
				</div>
				<div class='col fragment' style="padding-top: 1em">
					<img src="assets/images/speech_experiment.svg" alt="">
				</div>
			</div>
			<p class="fragment" style="font-size: 24px; width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
				Music and Speech: "One-Two-Three-Many"
			</p>
	</section>

	<section>
		<h2>Research Questions</h2>
		<ul>
			<li style="opacity: 0.5">What are the limitations of subitizing audio sources?</li>
			<li class='confirm'>Can we build a machine to solve this tasks?</li>
			<li>Would a machine be subject to the same limitations?</li>
		</ul>
	</section>

	<section>
		<h2>Task definition</h2>
		<img src="assets/images/task.svg" alt="">
		<p class="fragment" style="font-size: 28px; width: 60%; margin: auto; background-color: orange; padding: 10px; color:white">
			$k$ = Maximum number of concurrent speaker
		</p>
	</section>

	<section>
		<h2>Data-Driven Count Estimation</h2>
		<div style="text-align: center">
			<img width="70%" src="assets/images/train.svg" alt="">
		</div>
		<ul>
			<li>
				Synthetically overlapped mixtures using LibriSpeech
			</li>
			<li>
				20.020 training items (55h) ground truth labels
				<ul>
					<li>1820 samples for $[0 ... 10]$ speakers</li>
					<li>$k=0$ from TUT Acoustic Scenes dataset</li>
				</ul>
			</li>
		</ul>
	</section>

		<section>
			<h2>Input and Output</h2>
			<img width="100%" src="assets/images/io.svg" alt="">
		</section>

		<section>
			<section>
				<h2>Architectures</h2>
				<img width="120%" src="assets/images/architectures.svg" alt="">
		
			</section>
			<section>
				<h2>Architectures</h2>
				<img width="120%" src="assets/images/networkoverview.svg" alt="">
		
			</section>
			<section>
				<h2>CRNN</h2>
				<img width="120%" src="assets/images/nn.svg" alt="">
			</section>
		</section>

		<section>
			<h2>Model selection</h2>
			<img src="assets/images/countnet_parameters.svg" alt="">
			<p class="fragment" style="width: 70%; margin: auto; background-color: orange; padding: 10px; color:white">
				CountNet = STFT > CRNN > Classification
			</p>
		</section>

		<section>
			<h2>Results on Test set</h2>
			<div class="container">
				<div class="col">
					<ul style="font-size: 24px">
						<li class="fragment">Excellent voice activity detection</li>
						<li class="fragment">Overestimation between $2 < k < 6$</li> <li class="fragment">Error rarely larger
								than $k=2$</li>
						</k>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/responses.svg" alt="">
				</div>
		
			</div>
		</section>

		<section>
			<h2>Complexity</h2>
			<div class="container">
				<div class="col">
					<ul style="font-size: 24px">
						<li class="fragment">CRNN: Best Performance/Complexity</li>
						<li class="fragment">F-CRNN: Suitable for mobile applications</li>
						</k>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/complexity.svg" alt="">
				</div>
			</div>
		</section>

		<section>
			<h2>Evaluation of CountNet</h2>

			<div class="container">
				<div class='col'>
					<div class="fig-container" data-file="assets/figures/original.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech
					</h4>

				</div>
				<div class='col fragment step-fade-in-then-out'>
				<ul style='font-size: 0.8em'>
					<li>MEAN: $k=5$</li>
					<li>VQ: K-Means of ${MFCC}[7]$</li>
					<li>SVC: SVM Classification ${MFCC}_{20}$</li>
					<li>SVR: SVM Regression ${MFCC}_{20}$</li>
					<li>RNN: DNN Recurrent LSTM on STFT</li>
					<li>CRNN: DNN LSTM+CNN on STFT</li>
				</ul>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/timit.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: TIMIT
					</h4>

				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/chinese.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: THCS10 ðŸ‡¨ðŸ‡³
					</h4>				
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/gains.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech +/- 6dB
					</h4>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/rev.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech</br>
						Test: LibriSpeech Reverberated
					</h4>
				</div>

				<div class='col fragment step-fade-in-then-out'>
					<div class="fig-container" data-file="assets/figures/revrev.html" data-style="height: 400px; width: 450px"></div>
					<h4>Train: LibriSpeech Reverberated</br>
						Test: LibriSpeech Reverberated
					</h4>
				</div>
			</div>				
		</section>

		<section>
			<h2>CountNet Demo</h2>
			<video controls loop>
				<source data-src="assets/video/countnet-demo.mp4" type="video/mp4" />
			</video>
		</section>

		<section>
			<h2>Research Questions</h2>
			<ul>
				<li style="opacity: 0.5">What are the limitations of subitizing audio sources?</li>
				<li style="opacity: 0.5">Can we build a machine to solve this tasks?</li>
				<li class='confirm'>Would a machine be subject to the same limitations?</li>
			</ul>
		</section>

		<section>
			<h2>CountNet vs. Human</h2>
			<div class="container">
				<div class="col">
					<ul style="font-size: 36px">
						<li class="man fragment">Underestimation</li>
						<li class="machine fragment">Overestimation </li>
						<li class="man fragment">One-Two-Three-Many</li>
						<li class="machine fragment">One-...-$k_{max}$</li>
					</ul>
				</div>
				<div class="col">
					<img width="100%" src="assets/images/superhuman.svg" alt="">
				</div>
			</div>
		</section>

		<section data-background="assets/images/blackbox.jpg" data-state="no-title-footer">
		
		</section>

		<section>
			<h2>Understanding CountNet</h2>
			<h3 style="margin-top: -20px">Convolutional Filters</h3>
			<img width="120%" src="assets/images/convfilters.svg" alt="">
			<p style="font-size: 24px; width: 50%; margin: auto; background-color: orange; padding: 10px; color:white">
				Does CountNet understand phonemes?
			</p>

		</section>

		<section>
			<h2>Understanding CountNet</h2>
			<h3 style="margin-top: -20px">Ablation Study</h3>
			<ul>
				<li>Using phonetic annotation in TIMIT dataset</li>	
				<li>Measure speaking rate</li>
				<li>Syllables per second => Modulation of Speech</li>
			</ul>
			<img width="90%" src="assets/images/phonemes.svg" alt="">
		</section>

		<section>
			<h2>Understanding CountNet</h2>

			<div class="container">
				<div class="col">
					<ul>
						<li>Retrained model on TIMIT</li>
						<li>Select k=6 items (balanced)</li>
						<li>Errors only <emph>underestimation</emph></li>
						<li>Significant effect of speaking rate</li>
						<li class="confirm">Speaking very slow may result in understimation</li>
					</ul>
				<p class="fragment" style="font-size: 24px; width: 50%; margin: auto; background-color: orange; margin-top: 20px; padding: 10px; color:white">
					CountNet used speech modulations
				</p>
				</div>

				<div class="col" style="flex: 0.5">
					<img width="70%" src="assets/images/speaking_rate.svg" alt="">
				</div>			
			</div>
		</section>

		
		<section>
			<h2>Conclusion</h2>
			<ul>
				<li class="fragment">Focus on overlapping part instead of non-overlapping part</li>
				<li class="fragment">Proposed methods for separating unison mixtures</li> 
				<li class="fragment">Extended Common Fate for the Music scenario</li>
				<li class="fragment">Speaker count estimation using DL</li>
				<li class="fragment">Listening tests showed performance better than human</li>
				<li class="confirm fragment">Modulations play an important role audio tasks</li>
			</ul>
		</section>

		<section>
			<h2>Outlook</h2>
			<ul>
				<li>Paradigm shift for signal processing</li>
				<li>DNN models for
					<ul>
						<li>Common fate representations</li>
						<li>Long term structure for slow modulations</li>
					</ul>
				</li>
				<li>Research on count estimation</li>
				<li>Extrapolation</li>
			</ul>
		</section>

	<section>
		<h2>Further Contributions</h2>
		<div class="container">
			<div class="col" style="flex: 2">
				<h3>Public Domain Data</h3>
				<ul>
					<li>Unison Dataset</li>
					<li>Cello Dataset</li>
					<li>DSD100, MUSDB18 Dataset</li>
				</ul>
				<h3>Reproducibility</h3>
				<ul>
					<li>commonfate</li>
					<li>musdb</li>
					<li>museval</li>
				</ul>
				<h3>Community</h3>
				<ul>
					<li>SiSEC Organization 2016, 2018</li>
					<li><a href="http://sisec17.audiolabs-erlangen.de">Interactive Evaluation</a></li>
				</ul>
	
			</div>
	
			<div class="col">
	
			</div>
		</div>
	</section>

	<section>
		<img width="120%" src="assets/images/trends.svg" alt="">
	</section>

	<section>
		<img width="120%" src="assets/images/duration.png" alt="">
	</section>

	</div>
	<div class='footer'>
		<div id="copyright">Fabian-Robert StÃ¶ter</br>PhD Defense</div>
		<img src="css/theme/alabs/alabs_logo.svg" alt="Logo" />
		<div id="middlebox">Separation and Count Estimation for Audio Sources</br> Overlapping in Time and Frequency</div>
	</div>
	</div>

	<script src="js/reveal.js"></script>

	<script>
		// More info about config & dependencies:
		// - https://github.com/hakimel/reveal.js#configuration
		// - https://github.com/hakimel/reveal.js#dependencies
		// var wavesurfer = WaveSurfer.create({
		// 		container: '#waveform',
		// 		waveColor: 'grey',
		// 		progressColor: 'orange'
		// 	});
		// 	wavesurfer.load('assets/audio/fm_sine.wav');

		Reveal.initialize({
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				TeX: {
					extensions: ["color.js"]
				}
			},
				// specified using percentage units.
			width: 960,
			height: 700,
			controls: false,
			progress: true,
			history: true,
			center: false,
			slideNumber: true,
			minScale: 0.1,
			maxScale: 5,
			transition: 'none', //
			audio: {
				prefix: 'audio/', 	// audio files are stored in the "audio" folder
				suffix: '.wav',		// audio files have the ".ogg" ending
				textToSpeechURL: null,  // the URL to the text to speech converter
				defaultNotes: false, 	// use slide notes as default for the text to speech converter
				defaultText: false, 	// use slide text as default for the text to speech converter
				advance: -10, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
				autoplay: true,	// automatically start slideshow
				defaultDuration: 10,	// default duration in seconds if no audio is available 
				defaultAudios: false,	// try to play audios with names such as audio/1.2.ogg
			},
			dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'node_modules/reveald3/reveald3.js' },
				{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function () { return !!document.body.classList; } },
				{ src: 'plugin/highlight/highlight.js', async: true }
			]
		});

	</script>
	</body>
</html>
